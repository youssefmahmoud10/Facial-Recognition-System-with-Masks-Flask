{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import datasets\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "from facenet_pytorch import MTCNN\n",
    "import cv2\n",
    "import time\n",
    "import embeddingModels\n",
    "import nbimporter\n",
    "import TripletFolderClass\n",
    "import torch.optim as optim\n",
    "from facenet_pytorch import InceptionResnetV1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mtcnn0 = MTCNN(image_size=150, margin=0, keep_all=False, min_face_size=40) # keep_all=False\n",
    "mtcnn = MTCNN(image_size=150, margin=0, keep_all=True, min_face_size=40) # keep_all=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionResnet(nn.Module):\n",
    "    def __init__(self, device, pool=None, dropout=0.3, pretrain=True):\n",
    "        super(InceptionResnet, self).__init__()\n",
    "        # fit an image, the output is a 512 embedding original\n",
    "        # the model is pre-trained on vggface2\n",
    "        if pretrain:\n",
    "            self.net = InceptionResnetV1(pretrained='vggface2', dropout_prob=dropout, device=device)\n",
    "        else:\n",
    "            self.net = InceptionResnetV1(dropout_prob=dropout, device=device)\n",
    "        # the number of channels in the output of convolutional layers\n",
    "        self.out_features = self.net.last_linear.in_features\n",
    "        # keep convolutional layers only and remove linear layers and global average pooling layer\n",
    "        if pool == 'gem':\n",
    "            self.net.avgpool_1a = GeM(p_trainable=True)\n",
    "    def forward(self, x):\n",
    "        # return a 512 dimension vector\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceNet(nn.Module):\n",
    "    def __init__(self, model_name=None, pool=None, dropout=0.0, embedding_size=512, device='cuda', pretrain=True):\n",
    "        super(FaceNet, self).__init__()\n",
    "        # Backbone\n",
    "        # three models choice 1. SE-ResNeXt101 2.EfficientNetB7 3.InceptionResnetV1 (Pre-trained for face recog.)\n",
    "        self.model_name = model_name\n",
    "\n",
    "        # model (backbone)\n",
    "        if(model_name=='resnet'):\n",
    "            self.model = SEResNeXt101(pretrain)\n",
    "        elif(model_name=='effnet'):\n",
    "            self.model = EfficientNetEncoderHead(depth=3, pretrain=pretrain)\n",
    "        else:\n",
    "            self.model = InceptionResnet(device, pool=pool, dropout=dropout, pretrain=pretrain)\n",
    "\n",
    "        # global pooling\n",
    "        if(pool == \"gem\"):\n",
    "            # Generalizing Pooling\n",
    "            self.global_pool = GeM(p_trainable=True)\n",
    "        else:\n",
    "            # global average pooling\n",
    "            self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        # neck\n",
    "        self.neck = nn.Sequential(\n",
    "                nn.Linear(self.model.out_features, embedding_size, bias=True),\n",
    "                nn.BatchNorm1d(embedding_size, eps=0.001),\n",
    "                #nn.Sigmoid()\n",
    "            )\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # backbone\n",
    "        if self.model_name == None:\n",
    "            return self.model(x)\n",
    "        \n",
    "        x = self.model(x)\n",
    "        # global pool\n",
    "        x = self.global_pool(x)\n",
    "        x = self.dropout(x)\n",
    "        # change the output from cnn to a vector first\n",
    "        x = x[:,:,0,0]\n",
    "        # neck\n",
    "        embeddings = self.neck(x)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "facenet = FaceNet(model_name=None, pool=None, embedding_size=512, dropout=0.3, device='cpu', pretrain='True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('./Models/InceptionResNetV1_Triplet.pth', map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inceptionresnet.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "MODEL_PATH = './Models/InceptionResNetV1_Triplet.pth'\n",
    "facenet.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facenet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "facenet.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = datasets.ImageFolder('Database') # photos folder path \n",
    "idx_to_class = {i:c for c,i in dataset.class_to_idx.items()} # accessing names of peoples from folder names\n",
    "\n",
    "def collate_fn(x):\n",
    "    return x[0]\n",
    "\n",
    "loader = DataLoader(dataset, collate_fn=collate_fn)\n",
    "\n",
    "name_list = [] # list of names corresponding to cropped photos\n",
    "embedding_list = [] # list of embeding matrix after conversion from cropped faces to embedding matrix using resnet\n",
    "\n",
    "for img, idx in loader:\n",
    "    face, prob = mtcnn0(img, return_prob=True)\n",
    "    if face is not None and prob>0.92:\n",
    "        emb = facenet(face.unsqueeze(0))\n",
    "        embedding_list.append(emb.detach())\n",
    "        name_list.append(idx_to_class[idx])\n",
    "\n",
    "# save data\n",
    "data = [embedding_list, name_list]\n",
    "torch.save(data, 'data.pt') # saving data.pt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Using webcam recognize face\n",
    "\n",
    "# loading data.pt file\n",
    "load_data = torch.load('data.pt')\n",
    "embedding_list = load_data[0]\n",
    "name_list = load_data[1]\n",
    "\n",
    "cam = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "while True:\n",
    "    ret, frame = cam.read()\n",
    "    if not ret:\n",
    "        print(\"failed to grab frame, try again\")\n",
    "        break\n",
    "        \n",
    "    img = Image.fromarray(frame)\n",
    "    img_cropped_list, prob_list = mtcnn(img, return_prob=True)\n",
    "    \n",
    "    if img_cropped_list is not None:\n",
    "        boxes, _ = mtcnn.detect(img)\n",
    "                \n",
    "        for i, prob in enumerate(prob_list):\n",
    "            if prob>0.90:\n",
    "                #img_cropped_list = img_cropped_list.to('cuda')\n",
    "                emb = facenet(img_cropped_list[i].unsqueeze(0)).detach() \n",
    "                \n",
    "                dist_list = [] # list of matched distances, minimum distance is used to identify the person\n",
    "                \n",
    "                for idx, emb_db in enumerate(embedding_list):\n",
    "                    #dist = torch.dist(emb, emb_db).item()\n",
    "                    dist = torch.linalg.norm(emb - emb_db).item()\n",
    "                    #pdist = torch.nn.PairwiseDistance(p=2)\n",
    "                    #dist = pdist(emb, emb_db)\n",
    "                    dist_list.append(dist)\n",
    "\n",
    "                min_dist = min(dist_list) # get minumum dist value\n",
    "                min_dist_idx = dist_list.index(min_dist) # get minumum dist index\n",
    "                name = name_list[min_dist_idx] # get name corrosponding to minimum dist\n",
    "                \n",
    "                box = boxes[i] \n",
    "                \n",
    "                original_frame = frame.copy() # storing copy of frame before drawing on it\n",
    "                \n",
    "                if min_dist<0.90:\n",
    "                    #similarity = ( 1 - min_dist) * 100\n",
    "                    #str_sim = \"{:.2f}\".format(similarity)\n",
    "                    \n",
    "                    frame = cv2.putText(frame, name+' '+ str(min_dist), (int(box[0]),int(box[1])), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0),1, cv2.LINE_AA)\n",
    "                \n",
    "                frame = cv2.rectangle(frame, (int(box[0]),int(box[1])) , (int(box[2]),int(box[3])), (255,0,0), 2)\n",
    "\n",
    "    cv2.imshow(\"FaceRec\", frame)\n",
    "        \n",
    "    \n",
    "    k = cv2.waitKey(1)\n",
    "    if k%256==27: # ESC\n",
    "        print('Esc pressed, closing...')\n",
    "        break\n",
    "        \n",
    "    elif k%256==32: # space to save image\n",
    "        print('Enter your name :')\n",
    "        name = input()\n",
    "        \n",
    "        # create directory if not exists\n",
    "        if not os.path.exists('Database/'+name):\n",
    "            os.mkdir('Database/'+name)\n",
    "            \n",
    "        img_name = \"Database/{}/{}.jpg\".format(name, int(time.time()))\n",
    "        cv2.imwrite(img_name, original_frame)\n",
    "        print(\" saved: {}\".format(img_name))\n",
    "        \n",
    "        \n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
